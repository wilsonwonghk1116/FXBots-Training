#!/usr/bin/env python3
"""
STABLE 85% VRAM Trainer - RTX 3090 24GB Optimized
Designed to successfully complete training with memory management
"""

import os
import sys
import time
import torch
import torch.nn as nn
import numpy as np
import pandas as pd
import random
import json
import logging
import psutil
import GPUtil
from datetime import datetime
from typing import Dict, List, Tuple, Optional
import gymnasium as gym
from gymnasium import spaces
import warnings
warnings.filterwarnings('ignore')

# Configure logging
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

class StableForexEnvironment(gym.Env):
    """Stable Forex Environment optimized for complete training"""
    
    def __init__(self):
        super().__init__()
        self.initial_balance = 10000.0
        self.balance = self.initial_balance
        self.position = 0
        self.trades = []
        self.balance_history = []
        self.current_step = 0
        self.max_steps = 2000
        
        # Generate synthetic dataset
        self.data = self._generate_stable_data()
        
        # Optimized observation space
        self.observation_space = spaces.Box(low=-np.inf, high=np.inf, shape=(100,), dtype=np.float32)
        self.action_space = spaces.Discrete(3)
        
        self.reset()
    
    def _generate_stable_data(self, length: int = 50000) -> np.ndarray:
        """Generate stable synthetic forex data"""
        logger.info(f"Generating stable forex dataset ({length} points)...")
        np.random.seed(42)
        
        start_price = 1.1000
        prices = [start_price]
        
        for i in range(length - 1):
            change = np.random.normal(0, 0.0005)
            trend = 0.000001 * np.sin(i / 100)
            new_price = prices[-1] + change + trend
            new_price = max(0.9000, min(1.3000, new_price))
            prices.append(new_price)
        
        return np.array(prices)
    
    def _get_observation(self) -> np.ndarray:
        """Get optimized observation"""
        if self.current_step < 100:
            obs = np.zeros(100)
            available_data = self.data[max(0, self.current_step-99):self.current_step+1]
            obs[-len(available_data):] = available_data[-100:]
        else:
            obs = self.data[self.current_step-99:self.current_step+1]
        
        if len(obs) > 1:
            obs = (obs - obs.mean()) / (obs.std() + 1e-8)
        
        return obs.astype(np.float32)
    
    def reset(self, seed: Optional[int] = None) -> Tuple[np.ndarray, Dict]:
        """Reset environment"""
        if seed:
            np.random.seed(seed)
        
        self.balance = self.initial_balance
        self.position = 0
        self.trades = []
        self.balance_history = [self.initial_balance]
        
        # FIXED: Set starting position with enough room for max_steps
        max_start = len(self.data) - self.max_steps - 100  # Leave buffer
        self.start_step = random.randint(100, max(100, max_start))
        self.current_step = self.start_step
        self.steps_taken = 0  # Track relative steps from start
        
        return self._get_observation(), {}
    
    def step(self, action: int) -> Tuple[np.ndarray, float, bool, bool, Dict]:
        """Execute trading step with improved reward mechanism"""
        # FIXED: Check proper termination conditions
        if self.current_step >= len(self.data) - 1 or self.steps_taken >= self.max_steps:
            return self._get_observation(), 0, True, False, {}
        
        current_price = self.data[self.current_step]
        next_price = self.data[self.current_step + 1]
        price_change = next_price - current_price
        
        reward = 0
        trade_executed = False
        
        # IMPROVED: Better trading logic with trade incentives
        if action == 1 and self.position <= 0:  # Buy action
            if self.position == -1:  # Close short position
                profit = -self.position * (current_price - getattr(self, 'entry_price', current_price)) * 10000
                reward += profit
                self.trades.append({
                    'type': 'close_short',
                    'entry_price': getattr(self, 'entry_price', current_price),
                    'exit_price': current_price,
                    'profit': profit,
                    'step': self.steps_taken
                })
                trade_executed = True
            
            # Open long position
            self.position = 1
            self.entry_price = current_price
            # Small reward for taking action (encourages trading)
            reward += 0.1
            
        elif action == 2 and self.position >= 0:  # Sell action
            if self.position == 1:  # Close long position
                profit = self.position * (current_price - getattr(self, 'entry_price', current_price)) * 10000
                reward += profit
                self.trades.append({
                    'type': 'close_long',
                    'entry_price': getattr(self, 'entry_price', current_price),
                    'exit_price': current_price,
                    'profit': profit,
                    'step': self.steps_taken
                })
                trade_executed = True
            
            # Open short position
            self.position = -1
            self.entry_price = current_price
            # Small reward for taking action (encourages trading)
            reward += 0.1
        
        # Unrealized P&L for open positions
        if self.position != 0:
            unrealized_pnl = self.position * (next_price - self.entry_price) * 10000
            reward += unrealized_pnl * 0.05  # Reduced to prevent over-optimization
        
        # ADDED: Small penalty for excessive holding to encourage more trading
        if action == 0:  # Hold action
            reward -= 0.01  # Small penalty to discourage excessive holding
        
        self.balance += reward * 0.01
        self.balance_history.append(self.balance)
        
        # FIXED: Properly increment both counters
        self.current_step += 1
        self.steps_taken += 1
        
        # FIXED: Check termination conditions properly
        done = self.steps_taken >= self.max_steps or self.current_step >= len(self.data) - 1
        
        return self._get_observation(), reward, done, False, {'trade_executed': trade_executed}
    
    def simulate_detailed(self, model, steps: int = 2000) -> Dict:
        """Detailed simulation for champion analysis with improved action selection"""
        self.reset()
        total_reward = 0
        device = next(model.parameters()).device
        action_counts = [0, 0, 0]  # Track action distribution
        entropy_bonus = 0  # Track exploration
        
        for step_num in range(steps):
            obs = self._get_observation()
            obs_tensor = torch.FloatTensor(obs).unsqueeze(0).to(device)
            
            with torch.no_grad():
                action_probs = model(obs_tensor)
                
                # IMPROVED: Add entropy calculation to encourage exploration
                entropy = -torch.sum(action_probs * torch.log(action_probs + 1e-8))
                entropy_bonus += entropy.item()
                
                # IMPROVED: Use probabilistic action selection with better exploration
                # Add small amount of noise to prevent extreme distributions
                noise = torch.randn_like(action_probs) * 0.01
                noisy_probs = torch.softmax(torch.log(action_probs + 1e-8) + noise, dim=-1)
                
                action_dist = torch.distributions.Categorical(noisy_probs)
                action = int(action_dist.sample().item())
                
                # Track action distribution for debugging
                action_counts[action] += 1
            
            obs, reward, done, _, info = self.step(action)
            
            # IMPROVED: Add entropy bonus to reward exploration
            reward += entropy.item() * 0.001  # Small bonus for diverse actions
            
            total_reward += reward
            
            if done:
                break
        
        # Log action distribution and entropy for debugging
        total_actions = sum(action_counts)
        if total_actions > 0:
            action_percentages = [count/total_actions*100 for count in action_counts]
            avg_entropy = entropy_bonus / total_actions if total_actions > 0 else 0
            logger.info(f"Action distribution - Hold: {action_percentages[0]:.1f}%, Buy: {action_percentages[1]:.1f}%, Sell: {action_percentages[2]:.1f}% (Avg Entropy: {avg_entropy:.3f})")
        
        # Calculate comprehensive metrics
        if len(self.trades) > 0:
            profits = [trade['profit'] for trade in self.trades]
            winning_trades = [p for p in profits if p > 0]
            losing_trades = [p for p in profits if p < 0]
            
            win_rate = len(winning_trades) / len(profits) if profits else 0
            avg_win = np.mean(winning_trades) if winning_trades else 0
            avg_loss = np.mean(losing_trades) if losing_trades else 0
            gross_profit = sum(winning_trades)
            gross_loss = abs(sum(losing_trades))
            profit_factor = gross_profit / gross_loss if gross_loss > 0 else float('inf')
            
            balance_array = np.array(self.balance_history)
            peak = np.maximum.accumulate(balance_array)
            drawdown = (peak - balance_array) / peak * 100
            max_drawdown = np.max(drawdown)
            recovery_factor = (self.balance - self.initial_balance) / max_drawdown if max_drawdown > 0 else 0
        else:
            win_rate = avg_win = avg_loss = gross_profit = gross_loss = profit_factor = max_drawdown = recovery_factor = 0
        
        return {
            'final_balance': self.balance,
            'total_return_pct': (self.balance - self.initial_balance) / self.initial_balance * 100,
            'total_trades': len(self.trades),
            'win_rate': win_rate,
            'avg_win': avg_win,
            'avg_loss': avg_loss,
            'gross_profit': gross_profit,
            'gross_loss': gross_loss,
            'profit_factor': profit_factor,
            'max_drawdown': max_drawdown,
            'recovery_factor': recovery_factor,
            'risk_reward_ratio': abs(avg_win / avg_loss) if avg_loss != 0 else 0,
            'trades': self.trades,
            'balance_history': self.balance_history,
            'action_counts': action_counts,
            'avg_entropy': avg_entropy if 'avg_entropy' in locals() else 0
        }

class StableTradingBot(nn.Module):
    """CHAMPIONSHIP TRADING BOT - Highly competitive neural network for 85% VRAM warfare"""
    
    def __init__(self, input_size: int = 100, hidden_size: int = 1536, output_size: int = 3, strategy_type: str = 'balanced'):
        super().__init__()
        
        # CHAMPIONSHIP ARCHITECTURE - Different fighting styles for competition
        self.strategy_type = strategy_type
        self.wins = 0  # Track victories against other bots
        self.battles = 0  # Track total battles
        
        # ULTRA-AGGRESSIVE sizing based on strategy type for maximum competition
        if strategy_type == 'ultra_aggressive':
            multiplier = 5  # MASSIVE networks for ultra-aggressive strategies
        elif strategy_type == 'aggressive':
            multiplier = 4  # Large networks for aggressive strategies
        elif strategy_type == 'scalper':
            multiplier = 4  # Large for quick decisions
        elif strategy_type == 'conservative':
            multiplier = 3  # Medium-large for conservative
        elif strategy_type == 'balanced':
            multiplier = 3  # Standard size
        elif strategy_type == 'contrarian':
            multiplier = 2  # Focused architecture
        else:  # momentum
            multiplier = 2
        
        # CHAMPIONSHIP LAYER SIZES - Much larger for intense competition
        first_layer = hidden_size * multiplier  # Up to 7680 for ultra-aggressive
        second_layer = hidden_size * multiplier // 2  
        third_layer = hidden_size * multiplier // 3   
        fourth_layer = hidden_size * multiplier // 4  
        fifth_layer = hidden_size * multiplier // 6
        
        # ULTRA-DEEP competitive architecture
        self.network = nn.Sequential(
            nn.Linear(input_size, first_layer),
            nn.ReLU(),
            nn.Dropout(0.3),
            nn.Linear(first_layer, second_layer),
            nn.ReLU(),
            nn.Dropout(0.2),
            nn.Linear(second_layer, third_layer),
            nn.ReLU(),
            nn.Dropout(0.2),
            nn.Linear(third_layer, fourth_layer),
            nn.ReLU(),
            nn.Dropout(0.1),
            nn.Linear(fourth_layer, fifth_layer),
            nn.ReLU(),
            nn.Dropout(0.1),
            nn.Linear(fifth_layer, fifth_layer // 2),
            nn.ReLU(),
            nn.Linear(fifth_layer // 2, output_size)
        )
        
        # CHAMPIONSHIP-SPECIFIC temperature for competitive diversity
        if strategy_type == 'ultra_aggressive':
            self.temperature = nn.Parameter(torch.tensor(0.3))  # ULTRA sharp decisions
        elif strategy_type == 'aggressive':
            self.temperature = nn.Parameter(torch.tensor(0.5))  # Sharp decisions
        elif strategy_type == 'scalper':
            self.temperature = nn.Parameter(torch.tensor(0.4))  # Quick sharp decisions
        elif strategy_type == 'conservative':
            self.temperature = nn.Parameter(torch.tensor(3.5))  # Very soft decisions
        elif strategy_type == 'contrarian':
            self.temperature = nn.Parameter(torch.tensor(1.0))  # Medium decisions
        elif strategy_type == 'momentum':
            self.temperature = nn.Parameter(torch.tensor(0.6))  # Focused decisions
        else:  # balanced
            self.temperature = nn.Parameter(torch.tensor(2.0))  # Current setting
        
        # COMPETITIVE initialization for championship warfare
        self._initialize_champion_weights()
    
    def _initialize_champion_weights(self):
        """Initialize weights for MAXIMUM COMPETITION between strategies"""
        for module in self.modules():
            if isinstance(module, nn.Linear):
                if self.strategy_type == 'ultra_aggressive':
                    nn.init.xavier_uniform_(module.weight, gain=1.5)  # MAXIMUM gain for ultra-aggressive
                elif self.strategy_type == 'aggressive':
                    nn.init.xavier_uniform_(module.weight, gain=1.2)  # High gain for aggressive
                elif self.strategy_type == 'scalper':
                    nn.init.xavier_uniform_(module.weight, gain=1.0)  # Fast reactions
                elif self.strategy_type == 'conservative':
                    nn.init.xavier_uniform_(module.weight, gain=0.2)  # Very low gain for conservative
                elif self.strategy_type == 'contrarian':
                    nn.init.xavier_uniform_(module.weight, gain=0.9)
                elif self.strategy_type == 'momentum':
                    nn.init.xavier_uniform_(module.weight, gain=0.7)
                else:  # balanced
                    nn.init.xavier_uniform_(module.weight, gain=0.5)
                
                if module.bias is not None:
                    if module == list(self.modules())[-2]:  # Final layer
                        if self.strategy_type == 'contrarian':
                            nn.init.constant_(module.bias, 1.0)  # Strong bias for contrarian
                        elif self.strategy_type == 'ultra_aggressive':
                            nn.init.constant_(module.bias, -0.5)  # Negative bias for ultra-aggressive
                        else:
                            nn.init.constant_(module.bias, 0.0)
                    else:
                        nn.init.constant_(module.bias, 0.0)
    
    def record_battle_result(self, won: bool):
        """Record battle results for championship ranking"""
        self.battles += 1
        if won:
            self.wins += 1
    
    def get_win_rate(self) -> float:
        """Get championship win rate"""
        return self.wins / max(self.battles, 1)
    
    def forward(self, x):
        """Forward pass with CHAMPIONSHIP strategy-specific temperature scaling"""
        logits = self.network(x)
        # Apply ultra-competitive temperature scaling
        scaled_logits = logits / torch.clamp(self.temperature, min=0.1, max=5.0)
        probabilities = torch.softmax(scaled_logits, dim=-1)
        return probabilities

class Stable85PercentTrainer:
    """Stable trainer for 85% VRAM with memory management"""
    
    def __init__(self):
        self.device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
        self.env = StableForexEnvironment()
        
        # AGGRESSIVE SCALING for 85% VRAM utilization on RTX 3090 24GB
        # Previous test: 200 bots = 90% VRAM, so 190 bots should give us ~85%
        self.population_size = 190  # Optimized for 85% VRAM target
        
        # COMPETITIVE strategy diversity parameters
        self.strategy_types = ['ultra_aggressive', 'aggressive', 'conservative', 'balanced', 'contrarian', 'momentum', 'scalper']
        self.bots_per_strategy = self.population_size // len(self.strategy_types)
        
        # CHAMPIONSHIP competition parameters
        self.tournament_size = 8  # Tournament selection for breeding
        self.elite_percentage = 0.15  # Top 15% survive each generation
        self.mutation_intensity = 0.15  # Higher mutation for more competition
        
        logger.info(f"ğŸ† CHAMPIONSHIP 85% VRAM TRAINER INITIALIZED ğŸ†")
        logger.info(f"Population size: {self.population_size} (TARGET: 85% VRAM = ~20GB)")
        logger.info(f"COMPETITIVE MODES: {len(self.strategy_types)} strategy types")
        logger.info(f"Championship rules: Top {self.elite_percentage*100}% survive, Tournament size: {self.tournament_size}")
        logger.info(f"ğŸ¥Š MAXIMUM COMPETITION MODE ACTIVATED ğŸ¥Š")
    
    def create_stable_population(self) -> List[StableTradingBot]:
        """Create LARGE population with strategy diversity for 85% VRAM"""
        logger.info(f"Creating MASSIVE population of {self.population_size} trading bots for 85% VRAM target...")
        population = []
        
        # Create diverse strategies
        for strategy_idx, strategy_type in enumerate(self.strategy_types):
            logger.info(f"Creating {self.bots_per_strategy} {strategy_type} bots...")
            
            for bot_idx in range(self.bots_per_strategy):
                bot = StableTradingBot(strategy_type=strategy_type).to(self.device)
                population.append(bot)
                
                # Monitor VRAM every 100 bots
                if len(population) % 100 == 0:
                    if torch.cuda.is_available():
                        gpu = GPUtil.getGPUs()[0]
                        vram_used = gpu.memoryUsed / 1024
                        vram_percent = gpu.memoryUsed / gpu.memoryTotal * 100
                        logger.info(f"Created {len(population)} bots, VRAM: {vram_used:.1f}GB ({vram_percent:.1f}%)")
                        
                        # Check if approaching 85% target
                        if vram_percent >= 80:
                            logger.info(f"ğŸ¯ APPROACHING TARGET: {vram_percent:.1f}% VRAM utilization!")
                        
                        # Memory management - clear cache periodically
                        if torch.cuda.is_available():
                            torch.cuda.empty_cache()
        
        # Final VRAM check
        if torch.cuda.is_available():
            gpu = GPUtil.getGPUs()[0]
            vram_used = gpu.memoryUsed / 1024
            vram_percent = gpu.memoryUsed / gpu.memoryTotal * 100
            logger.info(f"ğŸš€ POPULATION COMPLETE: {len(population)} bots, VRAM: {vram_used:.1f}GB ({vram_percent:.1f}%)")
            
            if vram_percent >= 80:
                logger.info("âœ… TARGET ACHIEVED: 80%+ VRAM utilization!")
            else:
                logger.info(f"âš ï¸  Below target: {vram_percent:.1f}% < 85% target")
        
        return population
    
    def evaluate_population(self, population: List[StableTradingBot]) -> List[Dict]:
        """CHAMPIONSHIP EVALUATION with head-to-head competition"""
        results = []
        
        logger.info("ğŸ¥Š CHAMPIONSHIP EVALUATION - MAXIMUM COMPETITION MODE ğŸ¥Š")
        
        for i, bot in enumerate(population):
            bot.eval()
            
            # Run multiple competitive simulations
            sim_results = []
            for sim in range(3):  # 3 rounds per bot for consistency
                metrics = self.env.simulate_detailed(bot, steps=1500)
                sim_results.append(metrics)
            
            # Calculate average performance across simulations
            avg_balance = np.mean([r['final_balance'] for r in sim_results])
            avg_trades = np.mean([r['total_trades'] for r in sim_results])
            avg_win_rate = np.mean([r['win_rate'] for r in sim_results])
            avg_profit_factor = np.mean([r['profit_factor'] for r in sim_results if r['profit_factor'] != float('inf')])
            
            # CHAMPIONSHIP SCORING with multiple factors
            performance_score = (
                avg_balance * 0.4 +  # Balance weight
                avg_trades * 0.1 +   # Activity bonus
                avg_win_rate * 1000 * 0.3 +  # Win rate bonus
                avg_profit_factor * 100 * 0.2  # Profit factor bonus
            )
            
            bot_result = {
                'bot_id': i,
                'strategy_type': bot.strategy_type,
                'final_balance': avg_balance,
                'total_trades': avg_trades,
                'win_rate': avg_win_rate,
                'profit_factor': avg_profit_factor,
                'championship_score': performance_score,
                'bot_win_rate': bot.get_win_rate()
            }
            results.append(bot_result)
            
            if i % 20 == 0:
                logger.info(f"ğŸ† Evaluated {i}/{len(population)} champions")
                # Periodic memory cleanup
                if torch.cuda.is_available():
                    torch.cuda.empty_cache()
        
        # Sort by championship score (higher is better)
        results = sorted(results, key=lambda x: x['championship_score'], reverse=True)
        
        # Log top performers
        logger.info("ğŸ† === TOP 5 CHAMPIONSHIP CONTENDERS ===")
        for i, result in enumerate(results[:5]):
            logger.info(f"#{i+1}: {result['strategy_type']} Bot {result['bot_id']} - "
                       f"Score: {result['championship_score']:.2f}, "
                       f"Balance: ${result['final_balance']:.2f}, "
                       f"Win Rate: {result['win_rate']:.3f}")
        
        return results
    
    def tournament_selection(self, population: List[StableTradingBot], results: List[Dict]) -> StableTradingBot:
        """TOURNAMENT SELECTION for competitive breeding"""
        # Select random contestants for tournament
        tournament_indices = random.sample(range(len(population)), self.tournament_size)
        tournament_results = [results[i] for i in tournament_indices]
        
        # Winner is the bot with highest championship score
        winner_result = max(tournament_results, key=lambda x: x['championship_score'])
        winner_bot = population[winner_result['bot_id']]
        
        return winner_bot
    
    def evolve_generation(self, population: List[StableTradingBot]) -> Tuple[List[StableTradingBot], List[Dict]]:
        """CHAMPIONSHIP EVOLUTION with tournament selection and aggressive competition"""
        results = self.evaluate_population(population)
        
        logger.info("ğŸ”¥ === CHAMPIONSHIP EVOLUTION PHASE === ğŸ”¥")
        
        # Elite selection - top performers survive
        elite_size = int(len(population) * self.elite_percentage)
        elite_bots = [population[result['bot_id']] for result in results[:elite_size]]
        
        logger.info(f"ğŸ† {elite_size} CHAMPIONS advance to next generation")
        logger.info(f"ğŸ’€ {len(population) - elite_size} bots eliminated by natural selection")
        
        # Clear memory before creating new generation
        if torch.cuda.is_available():
            torch.cuda.empty_cache()
        
        # Create new generation through CHAMPIONSHIP BREEDING
        new_population = elite_bots.copy()
        
        logger.info("ğŸ§¬ === CHAMPIONSHIP BREEDING PHASE === ğŸ§¬")
        
        while len(new_population) < self.population_size:
            # Tournament selection for parents
            parent1 = self.tournament_selection(population, results)
            parent2 = self.tournament_selection(population, results)
            
            # Ensure different parents for diversity
            retry_count = 0
            while parent1 == parent2 and retry_count < 5:
                parent2 = self.tournament_selection(population, results)
                retry_count += 1
            
            # Create championship offspring
            child = self._championship_crossover(parent1, parent2)
            child = self._championship_mutation(child)
            new_population.append(child)
            
            # Clean up after each breeding
            if len(new_population) % 10 == 0:
                if torch.cuda.is_available():
                    torch.cuda.empty_cache()
        
        logger.info(f"ğŸ† NEW CHAMPIONSHIP GENERATION: {len(new_population)} competitive bots created")
        
        return new_population[:self.population_size], results
    
    def _championship_crossover(self, parent1: StableTradingBot, parent2: StableTradingBot) -> StableTradingBot:
        """CHAMPIONSHIP CROSSOVER - Breed the strongest traits from champion parents"""
        # Child inherits the strategy of the stronger parent
        if parent1.get_win_rate() > parent2.get_win_rate():
            child_strategy = parent1.strategy_type
        else:
            child_strategy = parent2.strategy_type
            
        child = StableTradingBot(strategy_type=child_strategy).to(self.device)
        
        with torch.no_grad():
            for (p1_param, p2_param, child_param) in zip(
                parent1.parameters(), parent2.parameters(), child.parameters()
            ):
                # COMPETITIVE CROSSOVER - favor the stronger parent's genes more often
                if parent1.get_win_rate() > parent2.get_win_rate():
                    # Parent 1 is stronger - 70% chance to inherit from parent 1
                    mask = torch.rand_like(p1_param) < 0.7
                else:
                    # Parent 2 is stronger - 70% chance to inherit from parent 2  
                    mask = torch.rand_like(p1_param) > 0.7
                
                child_param.data.copy_(torch.where(mask, p1_param, p2_param))
        
        return child
    
    def _championship_mutation(self, bot: StableTradingBot) -> StableTradingBot:
        """CHAMPIONSHIP MUTATION - Aggressive mutations for competitive evolution"""
        with torch.no_grad():
            for param in bot.parameters():
                if torch.rand(1) < self.mutation_intensity:
                    # More aggressive mutation based on strategy type
                    if bot.strategy_type in ['ultra_aggressive', 'aggressive']:
                        mutation_strength = 0.02  # Higher mutation for aggressive bots
                    elif bot.strategy_type == 'scalper':
                        mutation_strength = 0.015  # Medium-high for scalpers
                    else:
                        mutation_strength = 0.01  # Standard mutation
                    
                    noise = torch.randn_like(param) * mutation_strength
                    param.add_(noise)
        
        return bot
    
    def analyze_champion(self, champion_bot: StableTradingBot, results: List[Dict]) -> Dict:
        """Comprehensive champion analysis with consistent environment"""
        best_metrics = results[0]
        
        # FIXED: Use a fresh environment instance for consistent analysis
        analysis_env = StableForexEnvironment()
        
        # FIXED: Use same steps as evaluation for consistency (instead of 5000)
        # Run multiple simulations for more robust analysis
        all_trades = []
        all_balances = []
        total_simulations = 5
        
        for sim in range(total_simulations):
            # Set a consistent seed for reproducible results
            analysis_env.reset(seed=42 + sim)
            detailed_metrics = analysis_env.simulate_detailed(champion_bot, steps=1500)
            all_trades.extend(detailed_metrics['trades'])
            all_balances.append(detailed_metrics['final_balance'])
        
        # Calculate averaged metrics from multiple simulations
        if len(all_trades) > 0:
            profits = [trade['profit'] for trade in all_trades]
            winning_trades = [p for p in profits if p > 0]
            losing_trades = [p for p in profits if p < 0]
            
            win_rate = len(winning_trades) / len(profits) if profits else 0
            avg_win = np.mean(winning_trades) if winning_trades else 0
            avg_loss = np.mean(losing_trades) if losing_trades else 0
            gross_profit = sum(winning_trades)
            gross_loss = abs(sum(losing_trades))
            profit_factor = gross_profit / gross_loss if gross_loss > 0 else float('inf')
            
            avg_balance = np.mean(all_balances)
            total_return_pct = (avg_balance - analysis_env.initial_balance) / analysis_env.initial_balance * 100
        else:
            win_rate = avg_win = avg_loss = gross_profit = gross_loss = profit_factor = 0
            avg_balance = analysis_env.initial_balance
            total_return_pct = 0
        
        logger.info(f"Champion analysis: {len(all_trades)} total trades across {total_simulations} simulations")
        
        analysis = {
            'champion_analysis': {
                'bot_id': best_metrics['bot_id'],
                'final_balance': avg_balance,
                'total_return_pct': total_return_pct,
                'win_rate': win_rate,
                'total_trades': len(all_trades),
                'gross_profit': gross_profit,
                'gross_loss': gross_loss,
                'profit_factor': profit_factor,
                'average_win': avg_win,
                'average_loss': avg_loss,
                'risk_reward_ratio': abs(avg_win / avg_loss) if avg_loss != 0 else 0,
                'max_drawdown': 0,  # Simplified for now
                'recovery_factor': 0,  # Simplified for now
                'sharpe_ratio': 0,  # Simplified for now  
                'calmar_ratio': 0,  # Simplified for now
                'trade_history': all_trades[:50],
                'balance_curve': all_balances,
                'simulations_count': total_simulations
            },
            'system_info': {
                'population_size': self.population_size,
                'device': str(self.device),
                'vram_optimization': 'Stable 85% RTX 3090 24GB',
                'timestamp': datetime.now().isoformat()
            }
        }
        
        return analysis
    
    def save_champion(self, bot: StableTradingBot, analysis: Dict) -> str:
        """Save champion bot and analysis"""
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        
        model_path = f"CHAMPION_BOT_STABLE85_{timestamp}.pth"
        torch.save(bot.state_dict(), model_path)
        
        analysis_path = f"CHAMPION_ANALYSIS_STABLE85_{timestamp}.json"
        with open(analysis_path, 'w') as f:
            json.dump(analysis, f, indent=2, default=str)
        
        logger.info(f"Champion saved: {model_path}")
        logger.info(f"Analysis saved: {analysis_path}")
        
        return model_path

def monitor_stable_vram():
    """Monitor stable VRAM usage"""
    if torch.cuda.is_available():
        gpu = GPUtil.getGPUs()[0]
        vram_used_gb = gpu.memoryUsed / 1024
        vram_total_gb = gpu.memoryTotal / 1024
        vram_percent = gpu.memoryUsed / gpu.memoryTotal * 100
        
        logger.info(f"ğŸ¯ VRAM: {vram_used_gb:.1f}GB / {vram_total_gb:.1f}GB ({vram_percent:.1f}%)")
        
        if vram_percent >= 80:
            logger.info("âœ… TARGET ACHIEVED: 80%+ VRAM utilization!")
        
        return vram_percent
    return 0

def main():
    """Main training with stable memory management"""
    logger.info("ğŸš€ === STABLE 85% VRAM TRAINING SYSTEM ===")
    logger.info("ğŸ¯ Optimized for RTX 3090 24GB - Target: 20GB VRAM (Stable)")
    
    trainer = Stable85PercentTrainer()
    
    # Create stable population
    population = trainer.create_stable_population()
    
    vram_percent = monitor_stable_vram()
    logger.info(f"ğŸ Population created - VRAM utilization: {vram_percent:.1f}%")
    
    # Stable training loop
    generations = 30
    for generation in range(generations):
        logger.info(f"\nğŸ”„ === Generation {generation + 1}/{generations} ===")
        vram_percent = monitor_stable_vram()
        
        # Evolve population
        population, results = trainer.evolve_generation(population)
        
        # Log best performer
        best = results[0]
        logger.info(f"ğŸ† Best Bot: Balance=${best['final_balance']:.2f}, "
                   f"Return={best['total_return_pct']:.2f}%, "
                   f"Win Rate={best['win_rate']:.2f}, "
                   f"Trades={best['total_trades']}")
        
        # Save champion every 5 generations
        if (generation + 1) % 5 == 0:
            champion_bot = population[best['bot_id']]
            analysis = trainer.analyze_champion(champion_bot, results)
            trainer.save_champion(champion_bot, analysis)
            
            champion = analysis['champion_analysis']
            logger.info(f"\nğŸ“Š === CHAMPION ANALYSIS (Gen {generation + 1}) ===")
            logger.info(f"   ğŸ’° Final Balance: ${champion['final_balance']:.2f}")
            logger.info(f"   ğŸ“ˆ Total Return: {champion['total_return_pct']:.2f}%")
            logger.info(f"   ğŸ¯ Win Rate: {champion['win_rate']:.2f}")
            logger.info(f"   ğŸ”¢ Total Trades: {champion['total_trades']}")
            logger.info(f"   ğŸ’ Profit Factor: {champion['profit_factor']:.2f}")
            logger.info(f"   ğŸ“‰ Max Drawdown: {champion['max_drawdown']:.2f}%")
            logger.info(f"   ğŸ“Š Sharpe Ratio: {champion['sharpe_ratio']:.2f}")
    
    # Final champion analysis
    logger.info(f"\nğŸ === FINAL STABLE CHAMPION ANALYSIS ===")
    final_results = trainer.evaluate_population(population)
    champion_bot = population[final_results[0]['bot_id']]
    final_analysis = trainer.analyze_champion(champion_bot, final_results)
    final_model_path = trainer.save_champion(champion_bot, final_analysis)
    
    champion = final_analysis['champion_analysis']
    final_vram = monitor_stable_vram()
    
    logger.info(f"")
    logger.info(f"ğŸ‰ STABLE TRAINING COMPLETE! ğŸ‰")
    logger.info(f"")
    logger.info(f"ğŸ† FINAL CHAMPION PERFORMANCE:")
    logger.info(f"   ğŸ’° Final Balance: ${champion['final_balance']:.2f}")
    logger.info(f"   ğŸ“ˆ Total Return: {champion['total_return_pct']:.2f}%")
    logger.info(f"   ğŸ¯ Win Rate: {champion['win_rate']:.2f}")
    logger.info(f"   ğŸ”¢ Total Trades: {champion['total_trades']}")
    logger.info(f"   ğŸ’ Profit Factor: {champion['profit_factor']:.2f}")
    logger.info(f"   ğŸ’µ Average Win: {champion['average_win']:.2f} pips")
    logger.info(f"   ğŸ’¸ Average Loss: {champion['average_loss']:.2f} pips")
    logger.info(f"   âš–ï¸  Risk/Reward: {champion['risk_reward_ratio']:.2f}")
    logger.info(f"   ğŸ“‰ Max Drawdown: {champion['max_drawdown']:.2f}%")
    logger.info(f"   ğŸ”„ Recovery Factor: {champion['recovery_factor']:.2f}")
    logger.info(f"   ğŸ“Š Sharpe Ratio: {champion['sharpe_ratio']:.2f}")
    logger.info(f"   ğŸ“ˆ Calmar Ratio: {champion['calmar_ratio']:.2f}")
    logger.info(f"")
    logger.info(f"ğŸ’¾ Champion Model: {final_model_path}")
    logger.info(f"ğŸ¯ Final VRAM Usage: {final_vram:.1f}% (Stable)")
    logger.info(f"")

if __name__ == "__main__":
    main() 