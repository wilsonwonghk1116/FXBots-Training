#!/usr/bin/env python3
"""
Enhanced Smart Real Training System for Forex Bots
Targets 95% VRAM utilization on RTX 3090 24GB with comprehensive champion analysis
"""

import os
import sys
import time
# Ray removed
import torch
import torch.nn as nn
import torch.optim as optim
import numpy as np
import pandas as pd
import random
import json
import logging
import psutil
import GPUtil
from datetime import datetime
from typing import Dict, List, Tuple, Optional
import gymnasium as gym
from gymnasium import spaces
import warnings
from torch.utils.tensorboard import SummaryWriter
warnings.filterwarnings('ignore')

# Initialize TensorBoard
writer = SummaryWriter('logs/smart_trading')

# Configure logging
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

class SmartForexEnvironment(gym.Env):
    """Enhanced Forex Environment with difficulty levels"""
    
    def __init__(self, data_file: str = "data/EURUSD_H1.csv", initial_balance: float = 10000.0):
        super().__init__()
        self.initial_balance = initial_balance
        self.difficulty = 0  # 0-4 difficulty levels
        self.data = np.array([])  # Initialize empty data array
        self.max_steps = 1000  # Initialize max_steps
        self.position = 0
        self.entry_price = None
        self.trades = []
        self.balance_history = []
        self.current_step = 0
    def set_difficulty(self, level: int):
        """Adjust environment difficulty (0=easy, 4=hard)"""
        self.difficulty = max(0, min(4, level))
        # Adjust market volatility based on difficulty
        self.volatility_multiplier = 1.0 + self.difficulty * 0.25
        # Reset core trading state without reinitializing data
        self.balance = self.initial_balance
        self.position = 0
        self.entry_price = None
        self.trades = []
        self.balance_history = []
        # Reset trading state
        self.position = 0  # -1: short, 0: neutral, 1: long
        self.entry_price = None
        self.trades = []
        self.balance_history = [self.initial_balance]
        self.current_step = 0
        
        # Risk management parameters
        self.trading_cost = 0.0002  # 2 pips spread
        self.stop_loss_pips = 30  # 30 pips stop loss
        self.take_profit_pips = 60  # 60 pips take profit 
        self.max_position_size = 0.1  # Max 10% of balance per trade
        
        # Ensure data exists
        if len(self.data) == 0:
            # Generate synthetic data if no data exists
            self.data = self._generate_synthetic_data()
        
        # Gym spaces
        self.observation_space = spaces.Box(low=-np.inf, high=np.inf, shape=(20,), dtype=np.float32)
        self.action_space = spaces.Discrete(3)  # 0: hold, 1: buy, 2: sell
        
        self.reset()
    
    def _load_data(self, data_file: str) -> np.ndarray:
        """Load forex data from CSV file"""
        try:
            if os.path.exists(data_file) and os.path.getsize(data_file) > 0:
                df = pd.read_csv(data_file)
                return df[['Close']].values.flatten()
            else:
                return np.array([])
        except Exception as e:
            logger.warning(f"Could not load data from {data_file}: {e}")
            return np.array([])
    
    def _generate_synthetic_data(self, length: int = 10000) -> np.ndarray:
        """Generate synthetic EUR/USD-like data"""
        logger.info("Generating synthetic forex data for training...")
        np.random.seed(42)
        
        # Start price around EUR/USD typical range
        start_price = 1.1000
        prices = [start_price]
        
        for i in range(length - 1):
            # Random walk with slight trend and volatility
            change = np.random.normal(0, 0.0005)  # ~50 pips volatility
            trend = 0.000001 * np.sin(i / 100)  # Long-term cycle
            new_price = prices[-1] + change + trend
            new_price = max(0.9000, min(1.3000, new_price))  # Realistic bounds
            prices.append(new_price)
        
        return np.array(prices)
    
    def _get_observation(self) -> np.ndarray:
        """Get current market observation"""
        if self.current_step < 20:
            # Pad with zeros for initial steps
            obs = np.zeros(20)
            available_data = self.data[max(0, self.current_step-19):self.current_step+1]
            obs[-len(available_data):] = available_data[-20:]
        else:
            obs = self.data[self.current_step-19:self.current_step+1]
        
        # Normalize prices
        if len(obs) > 1:
            obs = (obs - obs.mean()) / (obs.std() + 1e-8)
        
        return obs.astype(np.float32)
    
    def step(self, action: int) -> Tuple[np.ndarray, float, bool, bool, Dict]:
        """Execute one step in the environment"""
        if self.current_step >= len(self.data) - 1:
            return self._get_observation(), 0, True, False, {}
        
        current_price = self.data[self.current_step]
        next_price = self.data[self.current_step + 1]
        price_change = next_price - current_price
        
        reward = 0
        trade_executed = False
        
        # Execute action with risk controls
        if action[1] == 1 and self.position <= 0:  # Buy signal
            position_size = min(action[0], self.max_position_size)
            
            if self.position == -1:  # Close short position
                # Apply stop loss/take profit
                exit_price = current_price
                pnl_pips = -self.position * (current_price - self.entry_price) * 10000
                
                if pnl_pips >= self.take_profit_pips:
                    exit_price = self.entry_price + self.take_profit_pips/10000
                elif pnl_pips <= -self.stop_loss_pips:
                    exit_price = self.entry_price - self.stop_loss_pips/10000
                
                # Calculate P&L with trading cost
                profit_pips = -self.position * (exit_price - self.entry_price) * 10000 - 2 * self.trading_cost * 10000
                reward += profit_pips * position_size
                
                self.trades.append({
                    'type': 'close_short',
                    'entry_price': self.entry_price,
                    'exit_price': exit_price,
                    'profit': profit_pips,
                    'position_size': position_size,
                    'step': self.current_step
                })
            
            # Open long position
            self.position = 1
            self.entry_price = current_price
            trade_executed = True
            
        elif action == 2 and self.position >= 0:  # Sell
            if self.position == 1:  # Close long position
                reward += self.position * price_change * 10000  # Convert to pips
                self.trades.append({
                    'type': 'close_long',
                    'entry_price': getattr(self, 'entry_price', current_price),
                    'exit_price': current_price,
                    'profit': self.position * (current_price - getattr(self, 'entry_price', current_price)) * 10000,
                    'step': self.current_step
                })
            
            # Open short position
            self.position = -1
            self.entry_price = current_price
            trade_executed = True
        
        # Calculate unrealized P&L for open positions
        if self.position != 0:
            unrealized_pnl = self.position * (next_price - self.entry_price) * 10000
            reward += unrealized_pnl * 0.1  # Small reward for favorable unrealized P&L
        
        # Update balance
        self.balance += reward * 0.01  # Convert pips to balance change
        self.balance_history.append(self.balance)
        
        self.current_step += 1
        done = self.current_step >= min(len(self.data) - 1, self.max_steps)
        
        info = {
            'balance': self.balance,
            'position': self.position,
            'price': next_price,
            'trade_executed': trade_executed,
            'total_trades': len(self.trades)
        }
        
        return self._get_observation(), reward, done, False, info
    
    def reset(self, seed: Optional[int] = None) -> Tuple[np.ndarray, Dict]:
        """Reset environment"""
        if seed:
            np.random.seed(seed)
            random.seed(seed)
        
        self.balance = self.initial_balance
        self.position = 0
        self.trades = []
        self.balance_history = [self.initial_balance]
        self.current_step = random.randint(20, max(20, len(self.data) - self.max_steps - 1))
        
        return self._get_observation(), {}
    
    def simulate_trading_detailed(self, model, steps: int = 1000) -> Dict:
        """Detailed trading simulation for champion analysis"""
        self.reset()
        total_reward = 0
        device = next(model.parameters()).device  # Get model device
        
        for _ in range(steps):
            obs = self._get_observation()
            obs_tensor = torch.FloatTensor(obs).unsqueeze(0).to(device)
            
            with torch.no_grad():
                action_probs = model(obs_tensor)
                action = torch.argmax(action_probs).item()
            
            obs, reward, done, _, info = self.step(action)
            total_reward += reward
            
            if done:
                break
        
        # Calculate comprehensive metrics
        if len(self.trades) > 0:
            profits = [trade['profit'] for trade in self.trades]
            winning_trades = [p for p in profits if p > 0]
            losing_trades = [p for p in profits if p < 0]
            
            win_rate = len(winning_trades) / len(profits) if profits else 0
            avg_win = np.mean(winning_trades) if winning_trades else 0
            avg_loss = np.mean(losing_trades) if losing_trades else 0
            gross_profit = sum(winning_trades)
            gross_loss = abs(sum(losing_trades))
            profit_factor = gross_profit / gross_loss if gross_loss > 0 else float('inf')
            
            # Risk metrics
            balance_array = np.array(self.balance_history)
            peak = np.maximum.accumulate(balance_array)
            drawdown = (peak - balance_array) / peak * 100
            max_drawdown = np.max(drawdown)
            recovery_factor = (self.balance - self.initial_balance) / max_drawdown if max_drawdown > 0 else 0
        else:
            win_rate = avg_win = avg_loss = gross_profit = gross_loss = profit_factor = max_drawdown = recovery_factor = 0
        
        return {
            'final_balance': self.balance,
            'total_return_pct': (self.balance - self.initial_balance) / self.initial_balance * 100,
            'total_trades': len(self.trades),
            'win_rate': win_rate,
            'avg_win': avg_win,
            'avg_loss': avg_loss,
            'gross_profit': gross_profit,
            'gross_loss': gross_loss,
            'profit_factor': profit_factor,
            'max_drawdown': max_drawdown,
            'recovery_factor': recovery_factor,
            'risk_reward_ratio': abs(avg_win / avg_loss) if avg_loss != 0 else 0,
            'trades': self.trades,
            'balance_history': self.balance_history,
            'total_reward': total_reward
        }

class SmartTradingBot(nn.Module):
    """Enhanced neural network with LSTM for forex trading"""
    
    def __init__(self, input_size: int = 20, hidden_size: int = 512, output_size: int = 3):
        super().__init__()
        self.input_size = input_size  # Store input dimension
        self.hidden_size = hidden_size
        self.output_size = output_size
        self.lstm = nn.LSTM(
            input_size=input_size,
            hidden_size=hidden_size // 2,
            num_layers=2,
            batch_first=True,
            dropout=0.2
        )
        self.position_control = nn.Sequential(
            nn.Linear(hidden_size // 2, hidden_size // 4),
            nn.ReLU(),
            nn.Linear(hidden_size // 4, 1),
            nn.Sigmoid()
        )
        self.action_head = nn.Sequential(
            nn.Linear(hidden_size // 2, hidden_size // 2),
            nn.ReLU(),
            nn.Linear(hidden_size // 2, output_size),
            nn.Softmax(dim=-1)
        )
    
    def forward(self, x):
        # Add sequence dimension (batch_size=1, seq_len=1, input_size)
        x = x.unsqueeze(1)
        lstm_out, _ = self.lstm(x)
        last_out = lstm_out[:, -1, :]
        
        position_size = self.position_control(last_out)
        action_probs = self.action_head(last_out)
        
        return torch.cat([position_size, action_probs], dim=1)
    def forward(self, x):
        # Ensure input has correct dimensions
        if x.dim() == 1:
            x = x.unsqueeze(0)  # Add batch dimension if missing
            
        # Add sequence dimension (batch_size, seq_len=1, input_size)
        x = x.unsqueeze(1)
        
        # Verify input dimension matches LSTM expected input
        if x.size(-1) != self.input_size:
            # Pad or truncate input if necessary
            if x.size(-1) < self.input_size:
                device = x.device  # Get input tensor device
                padding = torch.zeros(x.size(0), 1, self.input_size - x.size(-1), device=device)
                x = torch.cat([x, padding], dim=-1)
            else:
                x = x[:, :, :self.input_size]
                
        lstm_out, _ = self.lstm(x)
        last_out = lstm_out[:, -1, :]
        
        position_size = torch.sigmoid(self.position_control(last_out))
        action_probs = torch.softmax(self.action_head(last_out), dim=1)
        
        return torch.cat([position_size, action_probs], dim=1)

class VRAMOptimizedTrainer:
    """VRAM-optimized trainer with parallel evaluation"""
    
    def __init__(self, target_vram_percent: float = 0.70):
        self.device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
        self.target_vram_percent = target_vram_percent
        torch.backends.cudnn.benchmark = True
        self.generation = 0  # Track training generations
        
        # Curriculum learning parameters
        self.current_difficulty = 0
        self.max_difficulty = 4
        self.difficulty_increase_interval = 5
        
        # Parallel evaluation setup
        self.num_workers = min(4, (os.cpu_count() or 1) - 1)
        self.env_pool = [SmartForexEnvironment() for _ in range(self.num_workers)]
        self.env = SmartForexEnvironment()
        
        # Calculate optimal batch size for target VRAM usage
        self.batch_size = self._calculate_optimal_batch_size()
        self.population_size = min(1000, self.batch_size // 4)
        
        logger.info(f"Initialized trainer with batch_size={self.batch_size}, population_size={self.population_size}")
    
    def _calculate_optimal_batch_size(self) -> int:
        """Calculate optimal batch size for target VRAM utilization"""
        if not torch.cuda.is_available():
            return 64
        
        gpu = GPUtil.getGPUs()[0]
        total_memory = gpu.memoryTotal * 1024 * 1024  # Convert to bytes
        target_memory = total_memory * self.target_vram_percent
        
        # Estimate memory per sample (conservative estimate)
        memory_per_sample = 50 * 1024  # 50KB per sample
        optimal_batch_size = int(target_memory / memory_per_sample)
        
        # Ensure reasonable bounds
        return max(64, min(optimal_batch_size, 8192))
    
    def create_population(self) -> List[SmartTradingBot]:
        """Create initial population of trading bots"""
        population = []
        for _ in range(self.population_size):
            bot = SmartTradingBot().to(self.device)
            population.append(bot)
        return population
    
    def evaluate_population(self, population: List[SmartTradingBot]) -> List[Dict]:
        """Evaluate entire population with detailed metrics"""
        results = []
        
        for i, bot in enumerate(population):
            bot.eval()
            metrics = self.env.simulate_trading_detailed(bot, steps=1000)
            metrics['bot_id'] = i
            results.append(metrics)
            
            if i % 50 == 0:
                logger.info(f"Evaluated bot {i}/{len(population)}")
        
        return sorted(results, key=lambda x: x['final_balance'], reverse=True)
    
    def genetic_crossover(self, parent1: SmartTradingBot, parent2: SmartTradingBot) -> SmartTradingBot:
        """Create offspring through genetic crossover"""
        child = SmartTradingBot().to(self.device)
        
        with torch.no_grad():
            for (name1, param1), (name2, param2), (name_child, param_child) in zip(
                parent1.named_parameters(), parent2.named_parameters(), child.named_parameters()
            ):
                # Random crossover mask
                mask = torch.rand_like(param1) > 0.5
                param_child.data = param1 * mask + param2 * (~mask)
        
        return child
    
    def mutate(self, bot: SmartTradingBot, mutation_rate: float = 0.1) -> SmartTradingBot:
        """Apply mutations to bot"""
        with torch.no_grad():
            for param in bot.parameters():
                if torch.rand(1) < mutation_rate:
                    noise = torch.randn_like(param) * 0.01
                    param.add_(noise)
        return bot
    
    def evolve_generation(self, population: List[SmartTradingBot], elite_size: int = 100) -> List[SmartTradingBot]:
        # Adjust difficulty based on curriculum learning
        if self.generation % self.difficulty_increase_interval == 0:
            self.current_difficulty = min(self.current_difficulty + 1, self.max_difficulty)
            for env in self.env_pool:
                env.set_difficulty(self.current_difficulty)
        """Evolve population to next generation"""
        # Evaluate current population
        results = self.evaluate_population(population)
        
        # Select elite bots
        elite_bots = [population[result['bot_id']] for result in results[:elite_size]]
        
        # Create next generation
        new_population = elite_bots.copy()  # Keep elite
        
        while len(new_population) < self.population_size:
            # Select parents from top 50%
            parent1 = random.choice(elite_bots[:elite_size//2])
            parent2 = random.choice(elite_bots[:elite_size//2])
            
            # Create and mutate offspring
            child = self.genetic_crossover(parent1, parent2)
            child = self.mutate(child)
            new_population.append(child)
        
        return new_population[:self.population_size], results
    
    def analyze_champion(self, champion_bot: SmartTradingBot, results: List[Dict]) -> Dict:
        """Comprehensive analysis of champion bot"""
        champion_metrics = results[0]  # Best performer
        
        # Additional detailed analysis
        detailed_metrics = self.env.simulate_trading_detailed(champion_bot, steps=5000)
        
        analysis = {
            'champion_analysis': {
                'bot_id': champion_metrics['bot_id'],
                'final_balance': detailed_metrics['final_balance'],
                'total_return_pct': detailed_metrics['total_return_pct'],
                'win_rate': detailed_metrics['win_rate'],
                'total_trades': detailed_metrics['total_trades'],
                'gross_profit': detailed_metrics['gross_profit'],
                'gross_loss': detailed_metrics['gross_loss'],
                'profit_factor': detailed_metrics['profit_factor'],
                'average_win': detailed_metrics['avg_win'],
                'average_loss': detailed_metrics['avg_loss'],
                'risk_reward_ratio': detailed_metrics['risk_reward_ratio'],
                'max_drawdown': detailed_metrics['max_drawdown'],
                'recovery_factor': detailed_metrics['recovery_factor'],
                'sharpe_ratio': self._calculate_sharpe_ratio(detailed_metrics['balance_history']),
                'calmar_ratio': self._calculate_calmar_ratio(detailed_metrics),
                'trade_history': detailed_metrics['trades'][:50],  # Last 50 trades
                'balance_curve': detailed_metrics['balance_history'][-500:]  # Last 500 points
            },
            'training_summary': {
                'population_size': self.population_size,
                'batch_size': self.batch_size,
                'target_vram_percent': self.target_vram_percent,
                'device': str(self.device),
                'timestamp': datetime.now().isoformat()
            }
        }
        
        return analysis
    
    def _calculate_sharpe_ratio(self, balance_history: List[float]) -> float:
        """Calculate Sharpe ratio"""
        if len(balance_history) < 2:
            return 0
        
        returns = np.diff(balance_history) / balance_history[:-1]
        if np.std(returns) == 0:
            return 0
        
        return np.mean(returns) / np.std(returns) * np.sqrt(252)  # Annualized
    
    def _calculate_calmar_ratio(self, metrics: Dict) -> float:
        """Calculate Calmar ratio"""
        if metrics['max_drawdown'] == 0:
            return 0
        
        annual_return = metrics['total_return_pct']
        return annual_return / metrics['max_drawdown']
    
    def save_champion(self, champion_bot: SmartTradingBot, analysis: Dict) -> str:
        """Save champion bot and analysis"""
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        
        # Save model
        model_path = f"CHAMPION_BOT_{timestamp}.pth"
        torch.save(champion_bot.state_dict(), model_path)
        
        # Save analysis
        analysis_path = f"CHAMPION_ANALYSIS_{timestamp}.json"
        with open(analysis_path, 'w') as f:
            json.dump(analysis, f, indent=2, default=str)
        
        logger.info(f"Champion bot saved: {model_path}")
        logger.info(f"Analysis saved: {analysis_path}")
        
        return model_path

def monitor_system_resources():
    """Monitor and log system resources"""
    # GPU monitoring with fallback
    gpu_info = "N/A"
    if torch.cuda.is_available():
        try:
            gpus = GPUtil.getGPUs()
            if len(gpus) > 0:
                gpu = gpus[0]
                gpu_usage = gpu.memoryUsed / gpu.memoryTotal * 100
                gpu_info = f"GPU VRAM: {gpu_usage:.1f}% ({gpu.memoryUsed}MB/{gpu.memoryTotal}MB)"
            else:
                gpu_info = "GPU: No GPU detected"
        except Exception as e:
            gpu_info = f"GPU monitoring error: {str(e)}"
    else:
        gpu_info = "GPU: CUDA not available"
    
    # CPU and RAM monitoring
    cpu_percent = psutil.cpu_percent(interval=1)
    memory = psutil.virtual_memory()
    logger.info(f"{gpu_info}, CPU: {cpu_percent:.1f}%, RAM: {memory.percent:.1f}%")

def main():
    """Main training loop"""
    from argparse import ArgumentParser
    
    parser = ArgumentParser()
    parser.add_argument('--standalone', action='store_true', help='Run in standalone mode')
    args = parser.parse_args([])  # Force standalone with empty args
    args.standalone = True  # Double ensure standalone mode
    
    # Disable Ray completely in standalone mode
    import os
    os.environ['RAY_DISABLE'] = '1'
    
    logger.info("=== Smart Real Training System Started ===")
    logger.info("Targeting 95% VRAM utilization on RTX 3090 24GB")
    
    # Initialize trainer
    trainer = VRAMOptimizedTrainer(target_vram_percent=0.85)
    logger.info("Initialized trainer in standalone mode")
    logger.info(f"Starting training with population size {trainer.population_size}")
    
    # Create initial population
    logger.info("Creating initial population...")
    population = trainer.create_population()
    
    # Training loop
    generations = 50
    for generation in range(generations):
        logger.info(f"\n=== Generation {generation + 1}/{generations} ===")
        monitor_system_resources()
        
        # Evolve population
        population, results = trainer.evolve_generation(population)
        # Log best performer with detailed metrics
        best = max(results, key=lambda x: x.get('total_trades', 0) or x.get('final_balance', 0))
        trades = best.get('total_trades', 0)
        if trades == 0:
            logger.warning("No trades executed - adjusting strategy thresholds")
            # Adjust trading thresholds for next generation
            trainer.adjust_trading_thresholds(0.1)  # Increase sensitivity by 10%
            
        logger.info(f"Best Bot: Balance={best.get('final_balance', 0):.2f}, "
                   f"Return={best.get('total_return_pct', 0):.2f}%, "
                   f"Win Rate={best.get('win_rate', 0):.2f}, "
                   f"Trades={trades}, "
                   f"Avg Win={best.get('avg_win', 0):.1f}pips, "
                   f"Avg Loss={best.get('avg_loss', 0):.1f}pips, "
                   f"Profit Factor={best.get('profit_factor', 0):.2f}")
        if best['total_trades'] == 0:
            logger.warning("Warning: No trades executed - check strategy thresholds")
        
        # Log trading activity distribution
        action_counts = {'hold': 0, 'buy': 0, 'sell': 0}
        for r in results:
            if 'actions' in r:  # Ensure actions exist
                for action in r['actions']:
                    if action == 0: action_counts['hold'] += 1
                    elif action == 1: action_counts['buy'] += 1
                    elif action == 2: action_counts['sell'] += 1
            if 'actions' in r:
                for action in r['actions']:
                    if action == 0: action_counts['hold'] += 1
                    elif action == 1: action_counts['buy'] += 1
                    elif action == 2: action_counts['sell'] += 1
        logger.info(f"Action Distribution: Hold={action_counts['hold']}, Buy={action_counts['buy']}, Sell={action_counts['sell']}")
        
        # Analyze and save champion every 10 generations
        if (generation + 1) % 10 == 0:
            champion_bot = population[best['bot_id']]
            
            # Save model checkpoint
            checkpoint = {
                'generation': generation,
                'model_state_dict': champion_bot.state_dict(),
                'metrics': best
            }
            torch.save(checkpoint, f'checkpoints/gen_{generation}.pth')
            
            # Log metrics to TensorBoard
            writer.add_scalar('Balance', best['final_balance'], generation)
            writer.add_scalar('Return', best['total_return_pct'], generation)
            writer.add_scalar('Trades', best['total_trades'], generation)
            analysis = trainer.analyze_champion(champion_bot, results)
            model_path = trainer.save_champion(champion_bot, analysis)
            
            logger.info("\n=== CHAMPION ANALYSIS ===")
            champion = analysis['champion_analysis']
            logger.info(f"Final Balance: ${champion['final_balance']:.2f}")
            logger.info(f"Total Return: {champion['total_return_pct']:.2f}%")
            logger.info(f"Win Rate: {champion['win_rate']:.2f}")
            logger.info(f"Profit Factor: {champion['profit_factor']:.2f}")
            logger.info(f"Max Drawdown: {champion['max_drawdown']:.2f}%")
            logger.info(f"Sharpe Ratio: {champion['sharpe_ratio']:.2f}")
    
    # Final champion analysis
    logger.info("\n=== FINAL CHAMPION ANALYSIS ===")
    final_results = trainer.evaluate_population(population)
    champion_bot = population[final_results[0]['bot_id']]
    final_analysis = trainer.analyze_champion(champion_bot, final_results)
    final_model_path = trainer.save_champion(champion_bot, final_analysis)
    
    champion = final_analysis['champion_analysis']
    logger.info(f"🏆 CHAMPION BOT PERFORMANCE:")
    logger.info(f"   Final Balance: ${champion['final_balance']:.2f}")
    logger.info(f"   Total Return: {champion['total_return_pct']:.2f}%")
    logger.info(f"   Win Rate: {champion['win_rate']:.2f}")
    logger.info(f"   Total Trades: {champion['total_trades']}")
    logger.info(f"   Profit Factor: {champion['profit_factor']:.2f}")
    logger.info(f"   Average Win: {champion['average_win']:.2f} pips")
    logger.info(f"   Average Loss: {champion['average_loss']:.2f} pips")
    logger.info(f"   Risk/Reward: {champion['risk_reward_ratio']:.2f}")
    logger.info(f"   Max Drawdown: {champion['max_drawdown']:.2f}%")
    logger.info(f"   Recovery Factor: {champion['recovery_factor']:.2f}")
    logger.info(f"   Sharpe Ratio: {champion['sharpe_ratio']:.2f}")
    logger.info(f"   Calmar Ratio: {champion['calmar_ratio']:.2f}")
    logger.info(f"   Model saved: {final_model_path}")
    
    logger.info("\n=== Training Complete ===")

if __name__ == "__main__":
    main()
